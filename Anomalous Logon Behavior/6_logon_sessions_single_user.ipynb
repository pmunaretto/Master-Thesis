{"cells":[{"cell_type":"markdown","metadata":{"id":"v2M3HAr1_DcJ"},"source":["# Information\n","\n","**Author:**<br>Pascal Munaretto (<a href=\"mailto:pascal.munaretto@outlook.com\">Mail</a>)\n","\n","**Date:**<br>30.09.2022\n","\n","**Type:**<br>Master's Thesis\n","\n","**Topic:**<br>Design, Implementation and Performance Analysis of an AI-Based Insider Threat Detection Platform\tin Splunk To Counteract Data Exfiltration\n","\n","**Study Program:**<br>Enterprise and IT Security\n","\n","**Institution:**<br><a href=\"https://www.hs-offenburg.de\">Offenburg University of Applied Sciences</a>\n","\n","**Github:**<br>https://github.com/pmunaretto/Master-Thesis"]},{"cell_type":"markdown","metadata":{"id":"seeOSJP6ESdT"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"WtLVPWGk8-YN"},"source":["## Requirements"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rF3Z1A_XEOwJ"},"outputs":[],"source":["!pip install pyod suod"]},{"cell_type":"markdown","metadata":{"id":"VcYdcJW1QUNL"},"source":["## Patches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubi72YQ2QVQ3"},"outputs":[],"source":["# Add callbacks to Auto Encoder, VAE and Deep SVDD\n","!cp /content/drive/MyDrive/CERT/patches/patched_auto_encoder.py /usr/local/lib/python3.7/dist-packages/pyod/models/auto_encoder.py\n","!cp /content/drive/MyDrive/CERT/patches/patched_vae.py /usr/local/lib/python3.7/dist-packages/pyod/models/vae.py\n","!cp /content/drive/MyDrive/CERT/patches/patched_deep_svdd.py /usr/local/lib/python3.7/dist-packages/pyod/models/deep_svdd.py"]},{"cell_type":"markdown","metadata":{"id":"4e347ddb-6c45-49be-be90-4571d59910f3"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"77655bfa-86d1-4c65-9575-c63718f6c90f"},"outputs":[],"source":["import os\n","import math\n","import sys\n","import pandas as pd\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from timeit import default_timer as timer\n","from random import seed, randint\n","from sklearn.base import TransformerMixin, BaseEstimator, clone\n","from sklearn.compose import ColumnTransformer\n","from sklearn.model_selection import ParameterGrid\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.metrics import roc_auc_score, recall_score, f1_score, precision_score, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.compose import make_column_selector as selector\n","from matplotlib.backends.backend_pgf import FigureCanvasPgf\n","from matplotlib.ticker import PercentFormatter\n","from pyod.models.iforest import IForest\n","from pyod.models.ecod import ECOD\n","from pyod.models.copod import COPOD\n","from pyod.models.loda import LODA\n","from pyod.models.cblof import CBLOF\n","from pyod.models.pca import PCA\n","from pyod.models.auto_encoder import AutoEncoder\n","from pyod.models.vae import VAE\n","from pyod.models.deep_svdd import DeepSVDD\n","from IPython.display import display, Markdown"]},{"cell_type":"markdown","metadata":{"id":"01d5aada"},"source":["## Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68b27c5c"},"outputs":[],"source":["matplotlib.backend_bases.register_backend(\"pgf\", FigureCanvasPgf)\n","\n","plt.rcParams.update({\n","    \"figure.dpi\": 100,\n","    \"savefig.dpi\": 300,\n","    \"font.size\": 12,\n","    \"image.cmap\": \"plasma\",\n","    \"axes.prop_cycle\": plt.cycler(\"color\", \"bgrcmyk\"), \n","    \"pgf.texsystem\": \"pdflatex\",\n","    \"font.family\": \"serif\",\n","    \"text.usetex\": True,\n","    \"pgf.rcfonts\": False\n","})\n","\n","tf.get_logger().setLevel(\"WARN\")\n","\n","# Global Configuration\n","BASE_PATH     = \"/content/drive/MyDrive/CERT/r4.2\"\n","N_JOBS        = -1\n","N_ITER        = 5\n","CONTAMINATION = 0.01\n","RETRAIN       = False\n","DATASET_NAME  = \"logon_sessions_single_user\"\n","FEATURE_SETS  = [\n","    [\"session_duration\", \"hour\"],\n","    [\"session_duration\", \"weekday\"],\n","    [\"session_duration\", \"hour\", \"weekday\"],\n","    [\"session_duration\", \"hour_sin\", \"hour_cos\"],\n","    [\"session_duration\", \"weekday_sin\", \"weekday_cos\"],\n","    [\"session_duration\", \"hour_sin\", \"hour_cos\", \"weekday_sin\", \"weekday_cos\"]\n","]"]},{"cell_type":"markdown","metadata":{"id":"d137ae0d-a585-46f3-9cc2-9d33abfec4e9"},"source":["## Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9c1efc99"},"outputs":[],"source":["class Debugger(BaseEstimator, TransformerMixin):\n","\n","    def transform(self, data):\n","        print(\"Shape of Preprocessed data:\", data.shape)\n","        print(pd.DataFrame(data).head())\n","        return data\n","\n","    def fit(self, data, y=None, **fit_params):\n","        return self\n","\n","\n","def plot_anomaly_scores(series, identifier, min, max, save=True):\n","    plt.figure(figsize=(10,3))\n","    plt.hist(\n","        series,\n","        weights=np.ones(len(series)) / len(series),\n","        bins=np.arange(min, max, 0.02),\n","        rwidth=0.8\n","    )\n","    plt.xlim(xmin=min, xmax=max)\n","    plt.xticks(np.arange(min, max+0.1, 0.1))\n","    plt.xlabel(\"Anomaly Score\")\n","    plt.ylabel(\"Percentage\")\n","    plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n","    if save:\n","        plt.savefig(os.path.join(BASE_PATH, \"figures\", f\"{identifier}.pgf\"), format=\"pgf\")\n","    plt.show()\n","\n","\n","def plot_confusion_matrix(y_true, y_pred, identifier, save=True):\n","    ConfusionMatrixDisplay.from_predictions(\n","        y_true,\n","        y_pred,\n","        labels=[0, 1],\n","        display_labels=[\"Benign\", \"Malicious\"],\n","        values_format=\"d\",\n","        colorbar=True,\n","        cmap=\"plasma_r\"\n","    )\n","    plt.grid(False)\n","    if save:\n","        plt.savefig(os.path.join(BASE_PATH, \"figures\", f\"{identifier}.pgf\"), format=\"pgf\")\n","    plt.show()\n","\n","\n","def print_training_result(metrics):\n","    print(\n","        \"  \".join(\n","            [\n","                f\"\\033[1;33m Training Time: {metrics.training_time_avg:<7.4f}\\033[0m\",\n","                f\"\\033[1;33m Inference Time: {metrics.inference_time_avg:<7.4f}\\033[0m\",\n","                f\"\\033[1;35m pAUC: {metrics.p_auc_10_avg:02.4f} \\u00B1 {metrics.p_auc_10_std:02.4f}\\033[0m\",\n","                f\"\\033[1;35m Recall: {metrics.recall_avg:02.4f} \\u00B1 {metrics.recall_std:02.4f}\\033[0m\",\n","                f\"\\033[1;32m TN: {metrics.best_classifier_TN:<6}\\033[0m\",\n","                f\"\\033[1;31m FP: {metrics.best_classifier_FP:<5}\\033[0m\",\n","                f\"\\033[1;31m FN: {metrics.best_classifier_FN:<3}\\033[0m\",\n","                f\"\\033[1;32m TP: {metrics.best_classifier_TP:<3}\\033[0m\",\n","                f\"\\033[1;37m Params: {metrics.name}\\033[0m\"\n","            ]\n","        )\n","    )\n","\n","\n","def print_gridsearch_result(metrics):\n","    print(\n","        \"\\n\".join(\n","            [\n","                \"\\n\\033[4mBest hyperparameters:\\033[0m\",\n","                f\"Params: {metrics.name}\",\n","                f\"pAUC:   {metrics.p_auc_10_avg:02.4f} \\u00B1 {metrics.p_auc_10_std:02.4f}\",\n","                f\"Recall: {metrics.recall_avg:02.4f} \\u00B1 {metrics.recall_std:02.4f}\"\n","            ]\n","        )\n","    )\n","\n","\n","def calculate_dispersion_metrics_for_columns(source_df, destination_df, columns):\n","    for column in columns:\n","        avg = np.average(source_df[column])\n","        std = np.std(source_df[column])\n","        destination_df[f\"{column}_avg\"] = avg\n","        destination_df[f\"{column}_std\"] = std if not math.isnan(std) else 0\n","\n","    return destination_df\n","\n","\n","def train_classifier_on_single_users(df, classifier, features, params, n_iter=10):\n","    # Create a dataframe where the results of the different seeds will be stored\n","    random_state_summary = pd.DataFrame()\n","\n","    # Reset the PRNG seed\n","    seed(1)\n","\n","    # Group the dataframe by users\n","    grouped = df.groupby(\"user\", as_index=False)\n","\n","    # Perform the training process multiple times with random seeds\n","    for _ in range(n_iter):\n","\n","        # Create a dataframe where the results of the classifiers will be stored\n","        user_timings = pd.DataFrame()\n","        user_predictions = pd.DataFrame()\n","\n","        for name, group in grouped:\n","\n","            # Create a clone of the classifier\n","            try:\n","                classifier = clone(classifier)\n","            except:\n","                pass\n","            \n","            # Update the parameters of the classifier according to the grid search\n","            classifier.set_params(**params)\n","            \n","            # Set the random state attribute of the classifier (if it has one)\n","            try:\n","                classifier.set_params(**{\"random_state\": randint(0, 2**32)})\n","            except Exception:\n","                pass\n","\n","            # Define the transformers that do the rest of the preprocessing (scaling, encoding)\n","            numeric_transformer = Pipeline(steps=[\n","                (\"scaler\", RobustScaler())\n","            ])\n","            categorical_transformer = Pipeline(steps=[\n","                (\"ohe\", OneHotEncoder())\n","            ])\n","\n","            # Filter the dataframe according to the features \n","            group_filtered = group[features].copy()\n","\n","            # Create a pipeline that performs the feature selection and scaling\n","            pipe = Pipeline([\n","                (\"column_transformer\", ColumnTransformer(\n","                    transformers=[\n","                        (\"num\", numeric_transformer, selector(dtype_exclude=[\"category\", \"object\"])),\n","                        (\"cat\", categorical_transformer, selector(dtype_include=[\"category\", \"object\"]))\n","                    ]\n","                )),\n","                (\"classifier\", classifier)\n","            ])\n","        \n","            # Benchmark the training\n","            start_training = timer()\n","            pipe.fit(group_filtered)\n","            end_training = timer()\n","\n","            # Benchmark the inference\n","            start_inference = timer()\n","            pipe.predict(group_filtered)\n","            end_inference = timer()\n","\n","            # Add the predictions and anomaly scores to the group dataframe\n","            group[\"y_true\"] = group[\"threat\"]\n","            group[\"scores\"] = pipe.named_steps[\"classifier\"].decision_scores_\n","\n","            # Create a new series with the training metrics for the user iteration\n","            timings = pd.Series(\n","                {\n","                    \"training_time\": end_training - start_training,\n","                    \"inference_time\": end_inference - start_inference\n","                }\n","            )\n","\n","            # Append the series to our user summary dataframes\n","            user_timings = user_timings.append(timings, ignore_index=True)\n","            user_predictions = pd.concat([user_predictions, group])\n","\n","        # If there are inf values in the anomaly scores, replace it with the elsewise highest value\n","        max_without_infs = user_predictions[\"scores\"].replace([np.inf, -np.inf], np.nan).max()\n","        user_predictions[\"scores\"].replace(np.inf, max_without_infs, inplace=True)\n","        user_predictions[\"scores\"].replace(np.nan, 0, inplace=True)\n","\n","        # Find the right threshold to satisfy the contamination and add the final predictions\n","        upper = max_without_infs\n","        lower = 0\n","        for i in range(0,500):\n","            threshold = (upper + lower) / 2\n","            test = (user_predictions.scores > threshold).astype(bool)\n","            if test.sum() > CONTAMINATION * len(df):\n","                lower = threshold\n","            else:\n","                upper = threshold\n","        user_predictions[\"y_pred\"] = test\n","\n","        # After the predictions were added, calculate the evaluation metrics\n","        recall = recall_score(user_predictions.threat, user_predictions.y_pred)\n","        precision = precision_score(user_predictions.threat, user_predictions.y_pred)\n","        f1 = f1_score(user_predictions.threat, user_predictions.y_pred)\n","        cm = confusion_matrix(user_predictions.threat, user_predictions.y_pred, labels=[0, 1])\n","\n","        # Try to calculate the AUC, this could fail if one of the anomaly scores is infinite\n","        try:\n","            auc = roc_auc_score(user_predictions.threat, user_predictions.scores)\n","            p_auc_10 = roc_auc_score(user_predictions.threat, user_predictions.scores, max_fpr=0.1)\n","            p_auc_20 = roc_auc_score(user_predictions.threat, user_predictions.scores, max_fpr=0.2)\n","            p_auc_30 = roc_auc_score(user_predictions.threat, user_predictions.scores, max_fpr=0.3)\n","        except ValueError as e:\n","            print(e)\n","\n","        # Create a new series with all the information about the iteration\n","        metrics = pd.Series(\n","            {\n","                \"training_time\": user_timings.training_time.sum(),\n","                \"inference_time\": user_timings.inference_time.sum(),\n","                \"recall\": recall,\n","                \"precision\": precision,\n","                \"f1\": f1,\n","                \"TN\": cm[0][0],\n","                \"FP\": cm[0][1],\n","                \"FN\": cm[1][0],\n","                \"TP\": cm[1][1],\n","                \"auc\": auc,\n","                \"p_auc_10\": p_auc_10,\n","                \"p_auc_20\": p_auc_20,\n","                \"p_auc_30\": p_auc_30,\n","                \"y_true\": user_predictions.y_true,\n","                \"y_pred\": user_predictions.y_pred,\n","                \"scores\": user_predictions.scores,\n","            }\n","        )\n","\n","        # Append the series to our summary dataframe\n","        random_state_summary = random_state_summary.append(metrics, ignore_index=True)\n","\n","    # Convert the confusion matrix to integers\n","    random_state_summary = random_state_summary.astype({\"TN\": \"int32\", \"FP\": \"int32\", \"FN\": \"int32\", \"TP\": \"int32\"})\n","\n","    # Locate the best classifier and separate the predictions from it\n","    results = random_state_summary.iloc[random_state_summary.p_auc_10.argmax()].rename(str(params))\n","    predictions = results.loc[[\"y_true\", \"y_pred\", \"scores\"]]\n","\n","    # Remove the columns that should not be part of the results dataframe\n","    results.drop([\"y_true\", \"y_pred\", \"scores\"], inplace=True)\n","\n","    # Add the prefix\n","    results = results.add_prefix(\"best_classifier_\")\n","    \n","    # Add the average training and inference time to the dataframe\n","    results[\"training_time_avg\"]  = np.average(random_state_summary[\"training_time\"])\n","    results[\"inference_time_avg\"] = np.average(random_state_summary[\"inference_time\"])\n","\n","    # Calculate averages and different dispersion metrics for the best classifier series\n","    results = calculate_dispersion_metrics_for_columns(\n","        source_df=random_state_summary,\n","        destination_df=results,\n","        columns=[\"auc\", \"p_auc_10\", \"p_auc_20\", \"p_auc_30\", \"recall\"]\n","    )\n","\n","    return results, predictions\n","\n","\n","class GridSearch:\n","    def __init__(self, df, classifier, features, parameters, gridsearch_path):\n","\n","        # Instance variables\n","        self.df = df\n","        self.classifier = classifier\n","        self.features = features\n","        self.parameters = parameters\n","        self.gridsearch_path = gridsearch_path\n","\n","        # Main paths\n","        self.summary_path = os.path.join(self.gridsearch_path, \"gridsearch_summary.csv\")\n","        self.best_results_path = os.path.join(self.gridsearch_path, \"best_results.csv\")\n","        self.best_preds_path = os.path.join(self.gridsearch_path, \"best_preds.csv\")\n","\n","        # Create the output directory for the gridsearch\n","        os.makedirs(gridsearch_path, exist_ok=True)\n","\n","        # Read existing files\n","        if os.path.exists(self.summary_path) and not RETRAIN:\n","            self.gridsearch_summary = pd.read_csv(self.summary_path, index_col=0)\n","        else:\n","            self.gridsearch_summary = pd.DataFrame()\n","        if os.path.exists(self.best_results_path) and not RETRAIN:\n","            self.best_results = pd.read_csv(self.best_results_path, squeeze=True, index_col=0)\n","        else:\n","            self.best_results = None\n","\n","\n","    def start_training(self):\n","\n","        # Create an iterable parameter grid from the parameters dictionary\n","        grid = ParameterGrid(self.parameters)\n","\n","        # Debug output\n","        print(f\"\\033[4mTesting {len(list(grid))} different hyperparameter combinations\\033[0m\")\n","\n","        # Iterate over all possible parameter combinations\n","        for params in grid:\n","\n","            # Skip the parameters if they are already part of the gridsearch summary\n","            if not RETRAIN and not self.gridsearch_summary.empty and str(params) in self.gridsearch_summary.index:\n","                print_training_result(self.gridsearch_summary.loc[str(params)])\n","                continue\n","\n","            # Start the training process\n","            try: \n","                if hasattr(classifier, \"random_state\"):\n","                    results, predictions = train_classifier_on_single_users(\n","                        df=self.df,\n","                        classifier=self.classifier,\n","                        features=self.features,\n","                        params=params,\n","                        n_iter=N_ITER\n","                    )\n","                else:\n","                    results, predictions = train_classifier_on_single_users(\n","                        df=self.df,\n","                        classifier=self.classifier,\n","                        features=self.features,\n","                        params=params,\n","                        n_iter=1\n","                    )\n","            except ValueError as e:\n","                print(f\"Skipping {params}: {e}\")\n","                continue\n","\n","            # Print the metrics of the best classifier\n","            print_training_result(results)\n","\n","            # Add the results to the gridsearch summary\n","            self.gridsearch_summary = self.gridsearch_summary.append(results)\n","\n","            # Update the best classifier if the iterations performs better than the current best \n","            if self.best_results is None or results.p_auc_10_avg > self.best_results.p_auc_10_avg:\n","                self.best_results = results\n","                self.best_preds = predictions\n","                self.save_best_results()\n","\n","            # Save the progress\n","            self.save_gridsearch_summary()\n","\n","        # Print the results of the gridsearch (parameters with the best average)\n","        print_gridsearch_result(self.best_results)\n","\n","        return self.best_results\n","\n","\n","    def save_gridsearch_summary(self):\n","        self.gridsearch_summary.to_csv(self.summary_path)\n","\n","\n","    def save_best_results(self):\n","        self.best_results.to_csv(self.best_results_path)\n","        self.best_preds.to_frame()\\\n","            .transpose()\\\n","            .apply(pd.Series.explode)\\\n","            .reset_index(drop=True)\\\n","            .to_csv(self.best_preds_path, index=False)\n","\n","\n","    def get_summary(self):\n","        return self.gridsearch_summary\n","\n","\n","def initiate_training_run(classifier_name, classifier, parameters):\n","    # Define output paths\n","    summary_path = os.path.join(BASE_PATH, \"results_summary\", DATASET_NAME)\n","    summary_file = os.path.join(summary_path, \"summary.csv\")\n","\n","    # Create the output directory for the classifier\n","    os.makedirs(summary_path, exist_ok=True)\n","\n","    # Iterate through the feature sets\n","    for i, features in enumerate(FEATURE_SETS, start=6):\n","\n","        display(Markdown(f\"# {i}/{len(FEATURE_SETS)} - Features: {', '.join(features)}\"))\n","\n","        # Perform a grid search to find the best parameters for the classifier\n","        gridsearch = GridSearch(\n","            df=df,\n","            classifier=classifier,\n","            features=features,\n","            parameters=parameters,\n","            gridsearch_path=os.path.join(BASE_PATH, \"results_summary\", DATASET_NAME, classifier_name, f\"gridsearch{i}\")\n","        )\n","    \n","        best_parameter_series = gridsearch.start_training()\n","\n","        # Read the summary file if it already exists, otherwise create a new one\n","        if os.path.exists(summary_file):\n","            summary = pd.read_csv(summary_file, index_col=0)\n","        else:\n","            summary = pd.DataFrame()\n","\n","        # Set the index of the pandas series and update / append it to the summary\n","        index_name = f\"{classifier_name}_dataset{i}\"\n","        best_parameter_series.rename(index_name, inplace=True)\n","        if index_name in summary.index:\n","            summary.loc[index_name] = best_parameter_series\n","        else:\n","            summary = summary.append(best_parameter_series)\n","\n","        # Save the summaries and predictions to a file\n","        summary.sort_index(inplace=True)\n","        summary.to_csv(summary_file)"]},{"cell_type":"markdown","metadata":{"id":"57ef2b57-3b21-4b27-913d-c18dd2dbd93b"},"source":["## Loading the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"08762afc"},"outputs":[],"source":["# Read the dataset\n","df = pd.read_parquet(os.path.join(BASE_PATH, \"preprocessed\", \"logon_sessions\"))"]},{"cell_type":"markdown","metadata":{"id":"LLTWix1dOaO2"},"source":["# Training - Isolation Forest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mV-5Z8eMCcx4"},"outputs":[],"source":["# Local configuration\n","classifier_name = \"isolation_forest\"\n","\n","# Define the classifier that will be used for training\n","classifier = IForest(\n","    behaviour=\"new\",\n","    max_features=1.0,\n","    contamination=CONTAMINATION,\n","    n_jobs=N_JOBS\n",")\n","\n","# Define the hyperparameters grid that will be tested for best results\n","parameters = {\n","    \"n_estimators\": [1, 10, 50, 100],\n","    \"max_samples\": [128, 256, 512, 1024, 2048, 4096],\n","}\n","\n","# Start the training\n","initiate_training_run(classifier_name, classifier, parameters)"]},{"cell_type":"markdown","metadata":{"id":"ZV2Es8IADvCP"},"source":["# Training - LODA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J4Xq7TDkDvCQ"},"outputs":[],"source":["# Local configuration\n","classifier_name = \"loda\"\n","\n","# Define the classifier that will be used for training\n","classifier = LODA(\n","    contamination=CONTAMINATION\n",")\n","\n","# Define the hyperparameters grid that will be tested for best results\n","parameters = {\n","    \"n_bins\": [6, 8, 10, 12, 14, 16, 20],\n","    \"n_random_cuts\": [25, 50, 75, 100]\n","}\n","\n","# Start the training\n","initiate_training_run(classifier_name, classifier, parameters)"]},{"cell_type":"markdown","metadata":{"id":"-KEJkCq2_-9M"},"source":["# Training - COPOD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZadpCF4j_-9M"},"outputs":[],"source":["# Local configuration\n","classifier_name = \"copod\"\n","\n","# Define the classifier that will be used for training\n","classifier = COPOD(\n","    contamination=CONTAMINATION\n",")\n","\n","# Define the hyperparameters grid that will be tested for best results\n","parameters = {}\n","\n","# Start the training\n","initiate_training_run(classifier_name, classifier, parameters)"]},{"cell_type":"markdown","metadata":{"id":"iDnFM6VPjIsL"},"source":["# Training - ECOD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XnhXV6djIsN"},"outputs":[],"source":["# Local configuration\n","classifier_name = \"ecod\"\n","\n","# Define the classifier that will be used for training\n","classifier = ECOD(\n","    contamination=CONTAMINATION,\n","    n_jobs=1\n",")\n","\n","# Define the hyperparameters grid that will be tested for best results\n","parameters = {}\n","\n","# Start the training\n","initiate_training_run(classifier_name, classifier, parameters)"]},{"cell_type":"markdown","metadata":{"id":"bvWyGN-0ZiiE"},"source":["# Training - CBLOF"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KG3DE1MqZiiL"},"outputs":[],"source":["# Local configuration\n","classifier_name = \"cblof\"\n","\n","# Define the classifier that will be used for training\n","classifier = CBLOF(\n","    contamination=CONTAMINATION,\n","    n_jobs=N_JOBS\n",")\n","\n","# Define the hyperparameters grid that will be tested for best results\n","parameters = {\n","    \"n_clusters\": [2, 4],\n","    \"alpha\": [0.2, 0.4, 0.6, 0.8, 0.9],\n","    \"beta\": [2, 4, 8, 16],\n","    \"use_weights\": [True, False]\n","}\n","\n","# Start the training\n","initiate_training_run(classifier_name, classifier, parameters)"]},{"cell_type":"markdown","metadata":{"id":"-rCkC5Es1Dfo"},"source":["# Training - PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GV60Ethn1Dfx"},"outputs":[],"source":["# Local configuration\n","classifier_name = \"pca\"\n","\n","# Define the classifier that will be used for training\n","classifier = PCA(\n","    contamination=CONTAMINATION\n",")\n","\n","# Define the hyperparameters grid that will be tested for best results\n","parameters = {\n","    \"n_components\": [1, 2, 3, 4, 5, 6, 7],\n","    \"whiten\": [True, False],\n","    \"svd_solver\": [\"full\", \"arpack\", \"randomized\"],\n","    \"weighted\": [True, False],\n","    \"standardization\": [True, False]\n","}\n","\n","# Start the training\n","initiate_training_run(classifier_name, classifier, parameters)"]},{"cell_type":"markdown","metadata":{"id":"qVjUPw2SloEF"},"source":["# Training - AE"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"executionInfo":{"elapsed":76717361,"status":"ok","timestamp":1659005375429,"user":{"displayName":"Pascal Munaretto","userId":"15390274654486759460"},"user_tz":-120},"id":"7ienUhjJloEQ","outputId":"125dee6b-eace-474c-bf2b-595c912493da"},"outputs":[{"data":{"text/markdown":"# 6/1 - Features: session_duration, hour_sin, hour_cos, weekday_sin, weekday_cos","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\u001b[4mTesting 9 different hyperparameter combinations\u001b[0m\n","\u001b[1;33m Training Time: 4690.4049\u001b[0m  \u001b[1;33m Inference Time: 83.7789\u001b[0m  \u001b[1;35m pAUC: 0.7527 ± 0.0000\u001b[0m  \u001b[1;35m Recall: 0.0000 ± 0.0000\u001b[0m  \u001b[1;32m TN: 465787.0\u001b[0m  \u001b[1;31m FP: 4705.0\u001b[0m  \u001b[1;31m FN: 99.0\u001b[0m  \u001b[1;32m TP: 0.0\u001b[0m  \u001b[1;37m Params: {'hidden_activation': 'relu', 'hidden_neurons': [8, 4, 4, 8]}\u001b[0m\n","\u001b[1;33m Training Time: 5020.7474\u001b[0m  \u001b[1;33m Inference Time: 87.7416\u001b[0m  \u001b[1;35m pAUC: 0.7550 ± 0.0000\u001b[0m  \u001b[1;35m Recall: 0.0000 ± 0.0000\u001b[0m  \u001b[1;32m TN: 465786.0\u001b[0m  \u001b[1;31m FP: 4706.0\u001b[0m  \u001b[1;31m FN: 99.0\u001b[0m  \u001b[1;32m TP: 0.0\u001b[0m  \u001b[1;37m Params: {'hidden_activation': 'relu', 'hidden_neurons': [4, 2, 2, 4]}\u001b[0m\n","\u001b[1;33m Training Time: 4967.3995\u001b[0m  \u001b[1;33m Inference Time: 91.7561\u001b[0m  \u001b[1;35m pAUC: 0.7572 ± 0.0000\u001b[0m  \u001b[1;35m Recall: 0.0000 ± 0.0000\u001b[0m  \u001b[1;32m TN: 465787.0\u001b[0m  \u001b[1;31m FP: 4705.0\u001b[0m  \u001b[1;31m FN: 99.0\u001b[0m  \u001b[1;32m TP: 0.0\u001b[0m  \u001b[1;37m Params: {'hidden_activation': 'relu', 'hidden_neurons': [2, 1, 1, 2]}\u001b[0m\n","\u001b[1;33m Training Time: 5243.1731\u001b[0m  \u001b[1;33m Inference Time: 91.7551\u001b[0m  \u001b[1;35m pAUC: 0.7578 ± 0.0000\u001b[0m  \u001b[1;35m Recall: 0.0000 ± 0.0000\u001b[0m  \u001b[1;32m TN: 465787.0\u001b[0m  \u001b[1;31m FP: 4705.0\u001b[0m  \u001b[1;31m FN: 99.0\u001b[0m  \u001b[1;32m TP: 0.0\u001b[0m  \u001b[1;37m Params: {'hidden_activation': 'sigmoid', 'hidden_neurons': [8, 4, 4, 8]}\u001b[0m\n","\u001b[1;33m Training Time: 3744.1709\u001b[0m  \u001b[1;33m Inference Time: 75.7253\u001b[0m  \u001b[1;35m pAUC: 0.7520 ± 0.0000\u001b[0m  \u001b[1;35m Recall: 0.0000 ± 0.0000\u001b[0m  \u001b[1;32m TN: 465788\u001b[0m  \u001b[1;31m FP: 4704 \u001b[0m  \u001b[1;31m FN: 99 \u001b[0m  \u001b[1;32m TP: 0  \u001b[0m  \u001b[1;37m Params: {'hidden_activation': 'sigmoid', 'hidden_neurons': [4, 2, 2, 4]}\u001b[0m\n","\u001b[1;33m Training Time: 3787.7196\u001b[0m  \u001b[1;33m Inference Time: 74.5250\u001b[0m  \u001b[1;35m pAUC: 0.7542 ± 0.0000\u001b[0m  \u001b[1;35m Recall: 0.0000 ± 0.0000\u001b[0m  \u001b[1;32m TN: 465785\u001b[0m  \u001b[1;31m FP: 4707 \u001b[0m  \u001b[1;31m FN: 99 \u001b[0m  \u001b[1;32m TP: 0  \u001b[0m  \u001b[1;37m Params: {'hidden_activation': 'sigmoid', 'hidden_neurons': [2, 1, 1, 2]}\u001b[0m\n","\u001b[1;33m Training Time: 3847.9829\u001b[0m  \u001b[1;33m Inference Time: 79.1161\u001b[0m  \u001b[1;35m pAUC: 0.7519 ± 0.0000\u001b[0m  \u001b[1;35m Recall: 0.0000 ± 0.0000\u001b[0m  \u001b[1;32m TN: 465787\u001b[0m  \u001b[1;31m FP: 4705 \u001b[0m  \u001b[1;31m FN: 99 \u001b[0m  \u001b[1;32m TP: 0  \u001b[0m  \u001b[1;37m Params: {'hidden_activation': 'tanh', 'hidden_neurons': [8, 4, 4, 8]}\u001b[0m\n","\u001b[1;33m Training Time: 3850.6529\u001b[0m  \u001b[1;33m Inference Time: 78.4660\u001b[0m  \u001b[1;35m pAUC: 0.7518 ± 0.0000\u001b[0m  \u001b[1;35m Recall: 0.0000 ± 0.0000\u001b[0m  \u001b[1;32m TN: 465787\u001b[0m  \u001b[1;31m FP: 4705 \u001b[0m  \u001b[1;31m FN: 99 \u001b[0m  \u001b[1;32m TP: 0  \u001b[0m  \u001b[1;37m Params: {'hidden_activation': 'tanh', 'hidden_neurons': [4, 2, 2, 4]}\u001b[0m\n","\u001b[1;33m Training Time: 3874.7178\u001b[0m  \u001b[1;33m Inference Time: 73.7705\u001b[0m  \u001b[1;35m pAUC: 0.7514 ± 0.0000\u001b[0m  \u001b[1;35m Recall: 0.0000 ± 0.0000\u001b[0m  \u001b[1;32m TN: 465787\u001b[0m  \u001b[1;31m FP: 4705 \u001b[0m  \u001b[1;31m FN: 99 \u001b[0m  \u001b[1;32m TP: 0  \u001b[0m  \u001b[1;37m Params: {'hidden_activation': 'tanh', 'hidden_neurons': [2, 1, 1, 2]}\u001b[0m\n","\n","\u001b[4mBest hyperparameters:\u001b[0m\n","Params: {'hidden_activation': 'sigmoid', 'hidden_neurons': [8, 4, 4, 8]}\n","pAUC:   0.7578 ± 0.0000\n","Recall: 0.0000 ± 0.0000\n"]}],"source":["# Local configuration\n","classifier_name = \"auto_encoder\"\n","\n","# Define the classifier that will be used for training\n","classifier = AutoEncoder(\n","    output_activation=\"sigmoid\",\n","    optimizer=keras.optimizers.Adam(),\n","    epochs=100,\n","    batch_size=16384,\n","    validation_size=0.1,\n","    dropout_rate=0.2,\n","    l2_regularizer=0.1,\n","    preprocessing=False,\n","    verbose=0,\n","    callbacks=[\n","        keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, min_lr=1e-6),\n","        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6)\n","    ],\n","    contamination=CONTAMINATION\n",")\n","\n","# Define the hyperparameters grid that will be tested for best results\n","parameters = {\n","    \"hidden_neurons\": [[8, 4, 4, 8], [4, 2, 2, 4], [2, 1, 1, 2]],\n","    \"hidden_activation\": [\"relu\", \"sigmoid\", \"tanh\"]\n","}\n","\n","# Start the training\n","initiate_training_run(classifier_name, classifier, parameters)"]},{"cell_type":"markdown","metadata":{"id":"GLS-mcbtqK9-"},"source":["# Training - Deep SVDD"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"4h9dWlJXqK9_","outputId":"fff1b237-a6b8-47f6-ea67-bbfa19807b92"},"outputs":[{"data":{"text/markdown":"# 6/1 - Features: session_duration, hour_sin, hour_cos, weekday_sin, weekday_cos","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\u001b[4mTesting 36 different hyperparameter combinations\u001b[0m\n","Skipping {'hidden_activation': 'relu', 'hidden_neurons': [64, 32], 'use_ae': True}: The number of neurons should not exceed the number of features\n","\u001b[1;33m Training Time: 2964.3821\u001b[0m  \u001b[1;33m Inference Time: 70.4973\u001b[0m  \u001b[1;35m pAUC: 0.7481 ± 0.0000\u001b[0m  \u001b[1;35m Recall: 0.0303 ± 0.0000\u001b[0m  \u001b[1;32m TN: 465790.0\u001b[0m  \u001b[1;31m FP: 4702.0\u001b[0m  \u001b[1;31m FN: 96.0\u001b[0m  \u001b[1;32m TP: 3.0\u001b[0m  \u001b[1;37m Params: {'hidden_activation': 'relu', 'hidden_neurons': [64, 32], 'use_ae': False}\u001b[0m\n","Skipping {'hidden_activation': 'relu', 'hidden_neurons': [32, 16], 'use_ae': True}: The number of neurons should not exceed the number of features\n","Skipping {'hidden_activation': 'relu', 'hidden_neurons': [32, 16], 'use_ae': False}: Exception encountered when calling layer \"tf.math.subtract_2\" (type TFOpLambda).\n","\n","Dimensions must be equal, but are 16 and 32 for '{{node tf.math.subtract_2/Sub}} = Sub[T=DT_FLOAT](Placeholder, tf.math.subtract_2/Sub/y)' with input shapes: [?,16], [32].\n","\n","Call arguments received:\n","  • x=tf.Tensor(shape=(None, 16), dtype=float32)\n","  • y=array([0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n","       0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n","       0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n","       0.1       , 0.1       , 0.1       , 0.23998637, 0.1       ,\n","       0.14543046, 0.1       , 0.        , 0.13538507, 0.1       ,\n","       0.1       , 0.1964969 , 0.1       , 0.14778805, 0.1       ,\n","       0.10546836, 0.11347339], dtype=float32)\n","  • name=None\n","Skipping {'hidden_activation': 'relu', 'hidden_neurons': [16, 8], 'use_ae': True}: The number of neurons should not exceed the number of features\n","Skipping {'hidden_activation': 'relu', 'hidden_neurons': [16, 8], 'use_ae': False}: Exception encountered when calling layer \"tf.math.subtract_5\" (type TFOpLambda).\n","\n","Dimensions must be equal, but are 8 and 32 for '{{node tf.math.subtract_5/Sub}} = Sub[T=DT_FLOAT](Placeholder, tf.math.subtract_5/Sub/y)' with input shapes: [?,8], [32].\n","\n","Call arguments received:\n","  • x=tf.Tensor(shape=(None, 8), dtype=float32)\n","  • y=array([0.1       , 0.16109422, 0.1       , 0.13822757, 0.        ,\n","       0.1       , 0.1       , 0.1       , 0.1       , 0.        ,\n","       0.1       , 0.1       , 0.1       , 0.21843086, 0.1       ,\n","       0.1       , 0.1       , 0.12620272, 0.1       , 0.1       ,\n","       0.1       , 0.11924135, 0.1       , 0.1       , 0.1       ,\n","       0.12715316, 0.10468689, 0.1       , 0.11206969, 0.20873265,\n","       0.10814249, 0.1       ], dtype=float32)\n","  • name=None\n","Skipping {'hidden_activation': 'relu', 'hidden_neurons': [8, 4], 'use_ae': True}: Exception encountered when calling layer \"tf.math.subtract_10\" (type TFOpLambda).\n","\n","Dimensions must be equal, but are 4 and 32 for '{{node tf.math.subtract_10/Sub}} = Sub[T=DT_FLOAT](Placeholder, tf.math.subtract_10/Sub/y)' with input shapes: [?,4], [32].\n","\n","Call arguments received:\n","  • x=tf.Tensor(shape=(None, 4), dtype=float32)\n","  • y=array([0.1       , 0.16109417, 0.1       , 0.13822761, 0.        ,\n","       0.1       , 0.1       , 0.1       , 0.1       , 0.        ,\n","       0.1       , 0.1       , 0.1       , 0.21843094, 0.1       ,\n","       0.1       , 0.1       , 0.12620269, 0.1       , 0.1       ,\n","       0.1       , 0.11924134, 0.1       , 0.1       , 0.1       ,\n","       0.12715319, 0.1046869 , 0.1       , 0.11206973, 0.20873266,\n","       0.10814247, 0.1       ], dtype=float32)\n","  • name=None\n","Skipping {'hidden_activation': 'relu', 'hidden_neurons': [8, 4], 'use_ae': False}: Exception encountered when calling layer \"tf.math.subtract_13\" (type TFOpLambda).\n","\n","Dimensions must be equal, but are 4 and 32 for '{{node tf.math.subtract_13/Sub}} = Sub[T=DT_FLOAT](Placeholder, tf.math.subtract_13/Sub/y)' with input shapes: [?,4], [32].\n","\n","Call arguments received:\n","  • x=tf.Tensor(shape=(None, 4), dtype=float32)\n","  • y=array([0.1       , 0.16109416, 0.1       , 0.13822761, 0.        ,\n","       0.1       , 0.1       , 0.1       , 0.1       , 0.        ,\n","       0.1       , 0.1       , 0.1       , 0.21843094, 0.1       ,\n","       0.1       , 0.1       , 0.12620272, 0.1       , 0.1       ,\n","       0.1       , 0.11924132, 0.1       , 0.1       , 0.1       ,\n","       0.12715319, 0.1046869 , 0.1       , 0.11206967, 0.20873265,\n","       0.10814246, 0.1       ], dtype=float32)\n","  • name=None\n","Skipping {'hidden_activation': 'relu', 'hidden_neurons': [4, 2], 'use_ae': True}: Exception encountered when calling layer \"tf.math.subtract_18\" (type TFOpLambda).\n","\n","Dimensions must be equal, but are 2 and 32 for '{{node tf.math.subtract_18/Sub}} = Sub[T=DT_FLOAT](Placeholder, tf.math.subtract_18/Sub/y)' with input shapes: [?,2], [32].\n","\n","Call arguments received:\n","  • x=tf.Tensor(shape=(None, 2), dtype=float32)\n","  • y=array([0.1       , 0.16109419, 0.1       , 0.1382276 , 0.        ,\n","       0.1       , 0.1       , 0.1       , 0.1       , 0.        ,\n","       0.1       , 0.1       , 0.1       , 0.218431  , 0.1       ,\n","       0.1       , 0.1       , 0.12620272, 0.1       , 0.1       ,\n","       0.1       , 0.11924134, 0.1       , 0.1       , 0.1       ,\n","       0.12715319, 0.10468689, 0.1       , 0.11206965, 0.2087327 ,\n","       0.10814247, 0.1       ], dtype=float32)\n","  • name=None\n","Skipping {'hidden_activation': 'relu', 'hidden_neurons': [4, 2], 'use_ae': False}: Exception encountered when calling layer \"tf.math.subtract_21\" (type TFOpLambda).\n","\n","Dimensions must be equal, but are 2 and 32 for '{{node tf.math.subtract_21/Sub}} = Sub[T=DT_FLOAT](Placeholder, tf.math.subtract_21/Sub/y)' with input shapes: [?,2], [32].\n","\n","Call arguments received:\n","  • x=tf.Tensor(shape=(None, 2), dtype=float32)\n","  • y=array([0.1       , 0.16109417, 0.1       , 0.13822769, 0.        ,\n","       0.1       , 0.1       , 0.1       , 0.1       , 0.        ,\n","       0.1       , 0.1       , 0.1       , 0.21843086, 0.1       ,\n","       0.1       , 0.1       , 0.12620267, 0.1       , 0.1       ,\n","       0.1       , 0.11924135, 0.1       , 0.1       , 0.1       ,\n","       0.1271532 , 0.10468689, 0.1       , 0.1120697 , 0.20873266,\n","       0.10814242, 0.1       ], dtype=float32)\n","  • name=None\n","\u001b[1;33m Training Time: 4181.6578\u001b[0m  \u001b[1;33m Inference Time: 79.0959\u001b[0m  \u001b[1;35m pAUC: 0.5632 ± 0.0000\u001b[0m  \u001b[1;35m Recall: 0.1313 ± 0.0000\u001b[0m  \u001b[1;32m TN: 465799.0\u001b[0m  \u001b[1;31m FP: 4693.0\u001b[0m  \u001b[1;31m FN: 86.0\u001b[0m  \u001b[1;32m TP: 13.0\u001b[0m  \u001b[1;37m Params: {'hidden_activation': 'relu', 'hidden_neurons': [2, 1], 'use_ae': True}\u001b[0m\n","\u001b[1;33m Training Time: 3371.6115\u001b[0m  \u001b[1;33m Inference Time: 77.8964\u001b[0m  \u001b[1;35m pAUC: 0.5627 ± 0.0000\u001b[0m  \u001b[1;35m Recall: 0.1313 ± 0.0000\u001b[0m  \u001b[1;32m TN: 465799.0\u001b[0m  \u001b[1;31m FP: 4693.0\u001b[0m  \u001b[1;31m FN: 86.0\u001b[0m  \u001b[1;32m TP: 13.0\u001b[0m  \u001b[1;37m Params: {'hidden_activation': 'relu', 'hidden_neurons': [2, 1], 'use_ae': False}\u001b[0m\n","Skipping {'hidden_activation': 'sigmoid', 'hidden_neurons': [64, 32], 'use_ae': True}: The number of neurons should not exceed the number of features\n","\u001b[1;33m Training Time: 3106.6995\u001b[0m  \u001b[1;33m Inference Time: 74.3059\u001b[0m  \u001b[1;35m pAUC: 0.5789 ± 0.0000\u001b[0m  \u001b[1;35m Recall: 0.0707 ± 0.0000\u001b[0m  \u001b[1;32m TN: 465793.0\u001b[0m  \u001b[1;31m FP: 4699.0\u001b[0m  \u001b[1;31m FN: 92.0\u001b[0m  \u001b[1;32m TP: 7.0\u001b[0m  \u001b[1;37m Params: {'hidden_activation': 'sigmoid', 'hidden_neurons': [64, 32], 'use_ae': False}\u001b[0m\n","Skipping {'hidden_activation': 'sigmoid', 'hidden_neurons': [32, 16], 'use_ae': True}: The number of neurons should not exceed the number of features\n","Skipping {'hidden_activation': 'sigmoid', 'hidden_neurons': [32, 16], 'use_ae': False}: Exception encountered when calling layer \"tf.math.subtract_24\" (type TFOpLambda).\n","\n","Dimensions must be equal, but are 16 and 32 for '{{node tf.math.subtract_24/Sub}} = Sub[T=DT_FLOAT](Placeholder, tf.math.subtract_24/Sub/y)' with input shapes: [?,16], [32].\n","\n","Call arguments received:\n","  • x=tf.Tensor(shape=(None, 16), dtype=float32)\n","  • y=array([0.2878321 , 0.57438487, 0.6294026 , 0.6800352 , 0.36636823,\n","       0.43183053, 0.32450593, 0.3766117 , 0.46398592, 0.28696883,\n","       0.5596235 , 0.5179913 , 0.3861846 , 0.777227  , 0.5930365 ,\n","       0.5337092 , 0.6219369 , 0.5799909 , 0.6197209 , 0.40782285,\n","       0.35195985, 0.48790896, 0.5198926 , 0.52465963, 0.39007682,\n","       0.6551591 , 0.6125983 , 0.34222734, 0.669272  , 0.7403191 ,\n","       0.57741964, 0.603074  ], dtype=float32)\n","  • name=None\n","Skipping {'hidden_activation': 'sigmoid', 'hidden_neurons': [16, 8], 'use_ae': True}: The number of neurons should not exceed the number of features\n","Skipping {'hidden_activation': 'sigmoid', 'hidden_neurons': [16, 8], 'use_ae': False}: Exception encountered when calling layer \"tf.math.subtract_27\" (type TFOpLambda).\n","\n","Dimensions must be equal, but are 8 and 32 for '{{node tf.math.subtract_27/Sub}} = Sub[T=DT_FLOAT](Placeholder, tf.math.subtract_27/Sub/y)' with input shapes: [?,8], [32].\n","\n","Call arguments received:\n","  • x=tf.Tensor(shape=(None, 8), dtype=float32)\n","  • y=array([0.28783226, 0.5743849 , 0.6294024 , 0.6800352 , 0.36636832,\n","       0.4318306 , 0.32450604, 0.37661174, 0.46398598, 0.28696877,\n","       0.55962354, 0.51799124, 0.38618466, 0.77722645, 0.5930365 ,\n","       0.53370917, 0.6219369 , 0.5799911 , 0.6197205 , 0.40782288,\n","       0.35195994, 0.48790932, 0.51989275, 0.52465963, 0.3900769 ,\n","       0.65515876, 0.61259836, 0.3422273 , 0.669272  , 0.74031925,\n","       0.57741964, 0.6030737 ], dtype=float32)\n","  • name=None\n","Skipping {'hidden_activation': 'sigmoid', 'hidden_neurons': [8, 4], 'use_ae': True}: Exception encountered when calling layer \"tf.math.subtract_32\" (type TFOpLambda).\n","\n","Dimensions must be equal, but are 4 and 32 for '{{node tf.math.subtract_32/Sub}} = Sub[T=DT_FLOAT](Placeholder, tf.math.subtract_32/Sub/y)' with input shapes: [?,4], [32].\n","\n","Call arguments received:\n","  • x=tf.Tensor(shape=(None, 4), dtype=float32)\n","  • y=array([0.28783238, 0.574385  , 0.6294022 , 0.68003494, 0.36636838,\n","       0.43183047, 0.3245059 , 0.3766117 , 0.46398574, 0.2869688 ,\n","       0.55962324, 0.51799136, 0.38618466, 0.77722687, 0.5930361 ,\n","       0.53370893, 0.6219368 , 0.5799909 , 0.6197206 , 0.40782288,\n","       0.35195994, 0.4879087 , 0.5198925 , 0.52465945, 0.39007682,\n","       0.6551586 , 0.61259806, 0.34222737, 0.6692719 , 0.7403193 ,\n","       0.57741976, 0.60307384], dtype=float32)\n","  • name=None\n","Skipping {'hidden_activation': 'sigmoid', 'hidden_neurons': [8, 4], 'use_ae': False}: Exception encountered when calling layer \"tf.math.subtract_35\" (type TFOpLambda).\n","\n","Dimensions must be equal, but are 4 and 32 for '{{node tf.math.subtract_35/Sub}} = Sub[T=DT_FLOAT](Placeholder, tf.math.subtract_35/Sub/y)' with input shapes: [?,4], [32].\n","\n","Call arguments received:\n","  • x=tf.Tensor(shape=(None, 4), dtype=float32)\n","  • y=array([0.28783226, 0.5743849 , 0.6294026 , 0.6800352 , 0.36636832,\n","       0.43183053, 0.3245059 , 0.37661162, 0.46398586, 0.286969  ,\n","       0.5596234 , 0.5179913 , 0.3861846 , 0.7772267 , 0.59303665,\n","       0.5337093 , 0.62193704, 0.5799909 , 0.6197207 , 0.40782294,\n","       0.35195988, 0.48790923, 0.51989263, 0.5246594 , 0.39007682,\n","       0.65515906, 0.61259836, 0.34222734, 0.66927177, 0.7403195 ,\n","       0.57741964, 0.60307384], dtype=float32)\n","  • name=None\n","Skipping {'hidden_activation': 'sigmoid', 'hidden_neurons': [4, 2], 'use_ae': True}: Exception encountered when calling layer \"tf.math.subtract_40\" (type TFOpLambda).\n","\n","Dimensions must be equal, but are 2 and 32 for '{{node tf.math.subtract_40/Sub}} = Sub[T=DT_FLOAT](Placeholder, tf.math.subtract_40/Sub/y)' with input shapes: [?,2], [32].\n","\n","Call arguments received:\n","  • x=tf.Tensor(shape=(None, 2), dtype=float32)\n","  • y=array([0.28783226, 0.57438487, 0.6294024 , 0.68003523, 0.36636847,\n","       0.4318307 , 0.32450593, 0.37661147, 0.46398583, 0.286969  ,\n","       0.5596233 , 0.51799136, 0.3861847 , 0.777227  , 0.5930362 ,\n","       0.533709  , 0.6219368 , 0.57999104, 0.61972064, 0.40782312,\n","       0.35195985, 0.487909  , 0.51989263, 0.5246594 , 0.39007676,\n","       0.6551586 , 0.61259824, 0.34222722, 0.66927195, 0.74031895,\n","       0.5774197 , 0.6030738 ], dtype=float32)\n","  • name=None\n","Skipping {'hidden_activation': 'sigmoid', 'hidden_neurons': [4, 2], 'use_ae': False}: Exception encountered when calling layer \"tf.math.subtract_43\" (type TFOpLambda).\n","\n","Dimensions must be equal, but are 2 and 32 for '{{node tf.math.subtract_43/Sub}} = Sub[T=DT_FLOAT](Placeholder, tf.math.subtract_43/Sub/y)' with input shapes: [?,2], [32].\n","\n","Call arguments received:\n","  • x=tf.Tensor(shape=(None, 2), dtype=float32)\n","  • y=array([0.2878323 , 0.574385  , 0.62940246, 0.6800352 , 0.36636844,\n","       0.43183047, 0.3245059 , 0.37661156, 0.46398595, 0.28696895,\n","       0.5596235 , 0.51799154, 0.3861846 , 0.7772268 , 0.5930364 ,\n","       0.53370893, 0.6219369 , 0.579991  , 0.61972046, 0.40782285,\n","       0.35195994, 0.48790878, 0.51989245, 0.5246594 , 0.3900769 ,\n","       0.65515876, 0.6125984 , 0.34222722, 0.66927195, 0.74031913,\n","       0.5774199 , 0.6030742 ], dtype=float32)\n","  • name=None\n","\u001b[1;33m Training Time: 3802.2795\u001b[0m  \u001b[1;33m Inference Time: 75.6208\u001b[0m  \u001b[1;35m pAUC: 0.5387 ± 0.0000\u001b[0m  \u001b[1;35m Recall: 0.0303 ± 0.0000\u001b[0m  \u001b[1;32m TN: 465790.0\u001b[0m  \u001b[1;31m FP: 4702.0\u001b[0m  \u001b[1;31m FN: 96.0\u001b[0m  \u001b[1;32m TP: 3.0\u001b[0m  \u001b[1;37m Params: {'hidden_activation': 'sigmoid', 'hidden_neurons': [2, 1], 'use_ae': True}\u001b[0m\n","\u001b[1;33m Training Time: 3074.7848\u001b[0m  \u001b[1;33m Inference Time: 70.9265\u001b[0m  \u001b[1;35m pAUC: 0.5417 ± 0.0000\u001b[0m  \u001b[1;35m Recall: 0.0303 ± 0.0000\u001b[0m  \u001b[1;32m TN: 465788.0\u001b[0m  \u001b[1;31m FP: 4704.0\u001b[0m  \u001b[1;31m FN: 96.0\u001b[0m  \u001b[1;32m TP: 3.0\u001b[0m  \u001b[1;37m Params: {'hidden_activation': 'sigmoid', 'hidden_neurons': [2, 1], 'use_ae': False}\u001b[0m\n","Skipping {'hidden_activation': 'tanh', 'hidden_neurons': [64, 32], 'use_ae': True}: The number of neurons should not exceed the number of features\n","\u001b[1;33m Training Time: 3501.2962\u001b[0m  \u001b[1;33m Inference Time: 85.8260\u001b[0m  \u001b[1;35m pAUC: 0.7727 ± 0.0000\u001b[0m  \u001b[1;35m Recall: 0.2525 ± 0.0000\u001b[0m  \u001b[1;32m TN: 465877.0\u001b[0m  \u001b[1;31m FP: 4615.0\u001b[0m  \u001b[1;31m FN: 74.0\u001b[0m  \u001b[1;32m TP: 25.0\u001b[0m  \u001b[1;37m Params: {'hidden_activation': 'tanh', 'hidden_neurons': [64, 32], 'use_ae': False}\u001b[0m\n","Skipping {'hidden_activation': 'tanh', 'hidden_neurons': [32, 16], 'use_ae': True}: The number of neurons should not exceed the number of features\n","Skipping {'hidden_activation': 'tanh', 'hidden_neurons': [32, 16], 'use_ae': False}: Exception encountered when calling layer \"tf.math.subtract_46\" (type TFOpLambda).\n","\n","Dimensions must be equal, but are 16 and 32 for '{{node tf.math.subtract_46/Sub}} = Sub[T=DT_FLOAT](Placeholder, tf.math.subtract_46/Sub/y)' with input shapes: [?,16], [32].\n","\n","Call arguments received:\n","  • x=tf.Tensor(shape=(None, 16), dtype=float32)\n","  • y=array([-0.1       ,  0.1       , -0.1       , -0.1       ,  0.1       ,\n","       -0.1       , -0.1       , -0.1       , -0.1       ,  0.1       ,\n","       -0.1       , -0.1       ,  0.1       , -0.1       , -0.1       ,\n","        0.1       , -0.1       ,  0.1       ,  0.1       ,  0.1       ,\n","        0.1       ,  0.1       ,  0.1       , -0.1       ,  0.1       ,\n","       -0.1       ,  0.1       , -0.12477836,  0.1       , -0.1       ,\n","       -0.1       , -0.1       ], dtype=float32)\n","  • name=None\n","Skipping {'hidden_activation': 'tanh', 'hidden_neurons': [16, 8], 'use_ae': True}: The number of neurons should not exceed the number of features\n","Skipping {'hidden_activation': 'tanh', 'hidden_neurons': [16, 8], 'use_ae': False}: Exception encountered when calling layer \"tf.math.subtract_49\" (type TFOpLambda).\n","\n","Dimensions must be equal, but are 8 and 32 for '{{node tf.math.subtract_49/Sub}} = Sub[T=DT_FLOAT](Placeholder, tf.math.subtract_49/Sub/y)' with input shapes: [?,8], [32].\n","\n","Call arguments received:\n","  • x=tf.Tensor(shape=(None, 8), dtype=float32)\n","  • y=array([-0.1       ,  0.1       , -0.1       , -0.1       ,  0.1       ,\n","       -0.1       , -0.1       , -0.1       , -0.1       ,  0.1       ,\n","       -0.1       , -0.1       ,  0.1       , -0.1       , -0.1       ,\n","        0.1       , -0.1       ,  0.1       ,  0.1       ,  0.1       ,\n","        0.1       ,  0.1       ,  0.1       , -0.1       ,  0.1       ,\n","       -0.1       ,  0.1       , -0.12477827,  0.1       , -0.1       ,\n","       -0.1       , -0.1       ], dtype=float32)\n","  • name=None\n","Skipping {'hidden_activation': 'tanh', 'hidden_neurons': [8, 4], 'use_ae': True}: Exception encountered when calling layer \"tf.math.subtract_54\" (type TFOpLambda).\n","\n","Dimensions must be equal, but are 4 and 32 for '{{node tf.math.subtract_54/Sub}} = Sub[T=DT_FLOAT](Placeholder, tf.math.subtract_54/Sub/y)' with input shapes: [?,4], [32].\n","\n","Call arguments received:\n","  • x=tf.Tensor(shape=(None, 4), dtype=float32)\n","  • y=array([-0.1       ,  0.1       , -0.1       , -0.1       ,  0.1       ,\n","       -0.1       , -0.1       , -0.1       , -0.1       ,  0.1       ,\n","       -0.1       , -0.1       ,  0.1       , -0.1       , -0.1       ,\n","        0.1       , -0.1       ,  0.1       ,  0.1       ,  0.1       ,\n","        0.1       ,  0.1       ,  0.1       , -0.1       ,  0.1       ,\n","       -0.1       ,  0.1       , -0.12477832,  0.1       , -0.1       ,\n","       -0.1       , -0.1       ], dtype=float32)\n","  • name=None\n","Skipping {'hidden_activation': 'tanh', 'hidden_neurons': [8, 4], 'use_ae': False}: Exception encountered when calling layer \"tf.math.subtract_57\" (type TFOpLambda).\n","\n","Dimensions must be equal, but are 4 and 32 for '{{node tf.math.subtract_57/Sub}} = Sub[T=DT_FLOAT](Placeholder, tf.math.subtract_57/Sub/y)' with input shapes: [?,4], [32].\n","\n","Call arguments received:\n","  • x=tf.Tensor(shape=(None, 4), dtype=float32)\n","  • y=array([-0.1       ,  0.1       , -0.1       , -0.1       ,  0.1       ,\n","       -0.1       , -0.1       , -0.1       , -0.1       ,  0.1       ,\n","       -0.1       , -0.1       ,  0.1       , -0.1       , -0.1       ,\n","        0.1       , -0.1       ,  0.1       ,  0.1       ,  0.1       ,\n","        0.1       ,  0.1       ,  0.1       , -0.1       ,  0.1       ,\n","       -0.1       ,  0.1       , -0.12477827,  0.1       , -0.1       ,\n","       -0.1       , -0.1       ], dtype=float32)\n","  • name=None\n","Skipping {'hidden_activation': 'tanh', 'hidden_neurons': [4, 2], 'use_ae': True}: Exception encountered when calling layer \"tf.math.subtract_62\" (type TFOpLambda).\n","\n","Dimensions must be equal, but are 2 and 32 for '{{node tf.math.subtract_62/Sub}} = Sub[T=DT_FLOAT](Placeholder, tf.math.subtract_62/Sub/y)' with input shapes: [?,2], [32].\n","\n","Call arguments received:\n","  • x=tf.Tensor(shape=(None, 2), dtype=float32)\n","  • y=array([-0.1       ,  0.1       , -0.1       , -0.1       ,  0.1       ,\n","       -0.1       , -0.1       , -0.1       , -0.1       ,  0.1       ,\n","       -0.1       , -0.1       ,  0.1       , -0.1       , -0.1       ,\n","        0.1       , -0.1       ,  0.1       ,  0.1       ,  0.1       ,\n","        0.1       ,  0.1       ,  0.1       , -0.1       ,  0.1       ,\n","       -0.1       ,  0.1       , -0.12477833,  0.1       , -0.1       ,\n","       -0.1       , -0.1       ], dtype=float32)\n","  • name=None\n","Skipping {'hidden_activation': 'tanh', 'hidden_neurons': [4, 2], 'use_ae': False}: Exception encountered when calling layer \"tf.math.subtract_65\" (type TFOpLambda).\n","\n","Dimensions must be equal, but are 2 and 32 for '{{node tf.math.subtract_65/Sub}} = Sub[T=DT_FLOAT](Placeholder, tf.math.subtract_65/Sub/y)' with input shapes: [?,2], [32].\n","\n","Call arguments received:\n","  • x=tf.Tensor(shape=(None, 2), dtype=float32)\n","  • y=array([-0.1       ,  0.1       , -0.1       , -0.1       ,  0.1       ,\n","       -0.1       , -0.1       , -0.1       , -0.1       ,  0.1       ,\n","       -0.1       , -0.1       ,  0.1       , -0.1       , -0.1       ,\n","        0.1       , -0.1       ,  0.1       ,  0.1       ,  0.1       ,\n","        0.1       ,  0.1       ,  0.1       , -0.1       ,  0.1       ,\n","       -0.1       ,  0.1       , -0.12477826,  0.1       , -0.1       ,\n","       -0.1       , -0.1       ], dtype=float32)\n","  • name=None\n","\u001b[1;33m Training Time: 4615.3593\u001b[0m  \u001b[1;33m Inference Time: 90.9845\u001b[0m  \u001b[1;35m pAUC: 0.5723 ± 0.0000\u001b[0m  \u001b[1;35m Recall: 0.0404 ± 0.0000\u001b[0m  \u001b[1;32m TN: 465791.0\u001b[0m  \u001b[1;31m FP: 4701.0\u001b[0m  \u001b[1;31m FN: 95.0\u001b[0m  \u001b[1;32m TP: 4.0\u001b[0m  \u001b[1;37m Params: {'hidden_activation': 'tanh', 'hidden_neurons': [2, 1], 'use_ae': True}\u001b[0m\n","\u001b[1;33m Training Time: 4048.2701\u001b[0m  \u001b[1;33m Inference Time: 83.8076\u001b[0m  \u001b[1;35m pAUC: 0.5703 ± 0.0000\u001b[0m  \u001b[1;35m Recall: 0.0404 ± 0.0000\u001b[0m  \u001b[1;32m TN: 465790\u001b[0m  \u001b[1;31m FP: 4702 \u001b[0m  \u001b[1;31m FN: 95 \u001b[0m  \u001b[1;32m TP: 4  \u001b[0m  \u001b[1;37m Params: {'hidden_activation': 'tanh', 'hidden_neurons': [2, 1], 'use_ae': False}\u001b[0m\n","\n","\u001b[4mBest hyperparameters:\u001b[0m\n","Params: {'hidden_activation': 'tanh', 'hidden_neurons': [64, 32], 'use_ae': False}\n","pAUC:   0.7727 ± 0.0000\n","Recall: 0.2525 ± 0.0000\n"]}],"source":["# Local configuration\n","classifier_name = \"deep_svdd\"\n","\n","# Define the classifier that will be used for training\n","classifier = DeepSVDD(\n","    output_activation=\"sigmoid\",\n","    optimizer=keras.optimizers.Adam(),\n","    epochs=100,\n","    batch_size=16384,\n","    validation_size=0.1,\n","    dropout_rate=0.2,\n","    l2_regularizer=0.1,\n","    preprocessing=False,\n","    verbose=0,\n","    callbacks=[\n","        keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, min_lr=1e-6),\n","        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6)\n","    ],\n","    contamination=CONTAMINATION\n",")\n","\n","# Define the hyperparameters grid that will be tested for best results\n","parameters = {\n","    \"hidden_neurons\": [[64, 32], [32, 16], [16, 8], [8, 4], [4, 2], [2, 1]],\n","    \"hidden_activation\": [\"relu\", \"sigmoid\", \"tanh\"],\n","    \"use_ae\": [True, False]\n","}\n","\n","# Start the training\n","initiate_training_run(classifier_name, classifier, parameters)"]}],"metadata":{"colab":{"collapsed_sections":[],"toc_visible":true,"provenance":[],"mount_file_id":"1vMIvIiIJmD3ZgNEJ2H2beJFsYIYKNORu","authorship_tag":"ABX9TyOjncNVV2UwyuhZ4/sJbdXZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}